{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a43685ea",
   "metadata": {},
   "source": [
    "# Basic Variational Autoencoder (VAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8b862f",
   "metadata": {},
   "source": [
    "In this tutorial, we only focus on a simple VAE in PyTorch and visualize its latent representation training on the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030eed9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your machine to make sure you have all of the libraries we will be using.\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "#import torchvision.transforms as transforms\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from torchvision.utils import save_image, make_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca79207",
   "metadata": {},
   "source": [
    "Upload the MNIST dataset and make dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1430d5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a transform to apply to each datapoint. We can go further with adding a Grayscale\n",
    "# or normalization, but for now we'll stick to the critical one.\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# download the MNIST dataset if you don't have it already.\n",
    "data_path = './data'\n",
    "train_dataset = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n",
    "\n",
    "# create train and test dataloaders\n",
    "batch_size = 100\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972bc123",
   "metadata": {},
   "source": [
    "Let's visualize a sample of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6411ab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 25 sample training images for visualization\n",
    "dataiter = iter(train_loader)\n",
    "image = next(dataiter)\n",
    "\n",
    "num_samples = 25\n",
    "sample_images = [image[0][i,0] for i in range(num_samples)] \n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "grid = ImageGrid(fig, 111, nrows_ncols=(5, 5), axes_pad=0.1)\n",
    "\n",
    "for ax, im in zip(grid, sample_images):\n",
    "    ax.imshow(im, cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2350f94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e18d67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = image[1].reshape(image[1].shape[0], 1, 1, 1)\n",
    "dummy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2adc78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy2 = torch.nn.functional.one_hot(image[1], num_classes=10)\n",
    "torch.argmax(dummy2, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659a17f8",
   "metadata": {},
   "source": [
    "Now we can create a simple VAE using fully-connected encoders and decoders. The input dimension is 784, which is the flattened dimension of MNIST images (28x28). In the encoder, the mean ($\\mu$) and variance ($\\sigma^2$) vectors are our variational representation vectors (size=200). Notice that we multiply the latent variance with the epsilon ($\\epsilon$) parameter for reparameterization before decoding. This allows us to perform backpropagation and tackle the node stochastically.\n",
    "\n",
    "The basic reparameterization trick is as follows: $\\mathcal{N}(\\mu, \\sigma^2) \\sim \\sigma*\\epsilon+\\mu$ where $\\epsilon \\sim \\mathcal{N}(0, I_n)$\n",
    "\n",
    "Also, our final encoder dimension has a size of 2, which are the $\\mu$ and $\\sigma$ vectors. These continuous vectors define our latent distribution that allows us to sample images in the VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bb96d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    # Vanilla Variational Autoencoder\n",
    "    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=200, device=device):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),  # First layer transforms input of 784 to an intermediate of 400\n",
    "            nn.LeakyReLU(0.2),                 # LeakyReLU activation with a negative slope of 0.2\n",
    "            nn.Linear(hidden_dim, latent_dim), # Second layer transforms intermediate of 400 to an intermediate of 200\n",
    "            nn.LeakyReLU(0.2)                  # LeakyReLU activation with a negative slope of 0.2\n",
    "            )\n",
    "        \n",
    "        # latent mean and variance \n",
    "        self.mean_layer = nn.Linear(latent_dim, 2)    # Latent mean is found by transforming intermediate of 200 into a vector of 2\n",
    "        self.logvar_layer = nn.Linear(latent_dim, 2)  # Latent log-variance is found by transforming intermediate of 200 into a vector of 2\n",
    "        \n",
    "        # Quick note, the variance is referred to as the log variance because log(s^2) = 2*log(s).\n",
    "        # The idea here is to avoid producing a negative variance.\n",
    "        \n",
    "        # decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2, latent_dim),          # First layer transforms reparameterization of 2 to an intermediate of 200\n",
    "            nn.LeakyReLU(0.2),                 # LeakyReLU activation with a negative slope of 0.2\n",
    "            nn.Linear(latent_dim, hidden_dim), # Second layer transforms intermediate of 200 to an intermediate of 400\n",
    "            nn.LeakyReLU(0.2),                 # LeakyReLU activation with a negative slope of 0.2\n",
    "            nn.Linear(hidden_dim, input_dim),  # Third layer transforms intermediate of 400 to a reconstructed vector of 784\n",
    "            nn.Sigmoid()                       # Sigmoid activation function to keep vector values between [0,1]\n",
    "            )\n",
    "     \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)                                      # Pass the input into the encoder\n",
    "        mean, logvar = self.mean_layer(x), self.logvar_layer(x)  # Pass the output of the encoder into the mean and log-variance vectors\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterization(self, mean, std):\n",
    "        epsilon = torch.randn_like(std).to(device)  # Generates a vector from the N(0,1) distribution the same size of the variance vector\n",
    "        z = mean + std*epsilon                      # Reparameterization trick\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)   # Pass the input through the decoder.\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, logvar = self.encode(x)                             # Pass the input into the encoder to obtain the mean and log-variance vectors\n",
    "        z = self.reparameterization(mean, torch.exp(0.5*logvar))  # Perform reparameterization trick, but not before converting log-variance to standard deviation.\n",
    "        x_hat = self.decode(z)                                    # Pass the sample into the decoder to obtain reconstruction\n",
    "        return x_hat, mean, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b19e343",
   "metadata": {},
   "source": [
    "Now we can define our model and optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8e670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1aeb1d",
   "metadata": {},
   "source": [
    "The loss function of the VAE consists of reproduction loss and the Kullback-Leibler (KL) divergence. The KL divergence is a metric used to measure the distance between two probability distributions. KL divergence is an important concept in generative modelling.\n",
    "\n",
    "For reproduction loss, we use the Binary Cross Entropy to compare each feature of a data point to the value in the reconstructed output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31998ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(x, x_hat, mean, log_var):\n",
    "    reproduction_loss = nn.functional.binary_cross_entropy(x_hat, x, reduction='sum')\n",
    "    KLD = -0.5*torch.sum(1+log_var - mean.pow(2) - log_var.exp())\n",
    "    return reproduction_loss + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29e50f2",
   "metadata": {},
   "source": [
    "A quick note about the way the Kullback-Leibler divergence is being performed. Recall that we are interpreting the vectors output by the encoder as mean and log-variance. This is because if we interpretted the vectors and mean and variance, the model could output a negative value for the variance, which is not allowed. If we instead use log-variance, we can ensure that the variance will always be positive when transformed back.\n",
    "\n",
    "The Kullback-Leibler divergence is defined as the following:\n",
    "\n",
    "<center> $D_{KL}(p||q)=\\sum_{i=1}^{N}p(x_i)log(\\frac{p(x_i)}{q(x_i)})$ </center>\n",
    "\n",
    "Here, $q(x)$ is the approximation and $p(x)$ is the true distribution we're interested in matching $q(x)$ to $p(x)$. Intuitively, this measures how much a given arbitrary distribution is away from the true distribution. If two distributions match perfectly, then the KL-divergence is 0, otherwise it can take values between 0 and infinity. The lower the KL-divergence, the better we have matched the true distribution with our approximation.\n",
    "\n",
    "The KL divergence is programmed the way it is because we are measuring how close the latent representation is to a standard normal distribution. This lets us represent the KL-divergence as follows:\n",
    "\n",
    "<center> $D_{KL}(p(z|x)||q(z))=\\frac{1}{2}\\sum_{i=1}^{N}((\\mu_{\\phi}(x))^2_i+(\\sigma_{\\phi}(x))^2_i-ln((\\sigma_{\\phi}(x))^2_i)-1)$</center>\n",
    "\n",
    "Note how the KL-divergence is expressed in terms of variance. We can easily rewrite this function in terms of the log-variance ($\\varsigma$).\n",
    "\n",
    "<center> $D_{KL}(p(z|x)||q(z))=\\frac{1}{2}\\sum_{i=1}^{N}((\\mu_{\\phi}(x))^2_i+exp((\\varsigma_{\\phi}(x))_i)-(\\varsigma_{\\phi}(x))_i-1)$</center>\n",
    "\n",
    "Lastly, we can move the negative out in front to get the function that is programmed:\n",
    "\n",
    "<center> $D_{KL}(p(z|x)||q(z))=-\\frac{1}{2}\\sum_{i=1}^{N}(1+(\\varsigma_{\\phi}(x))_i-((\\mu_{\\phi}(x))^2_i)-exp((\\varsigma_{\\phi}(x))_i))$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4b6fe6",
   "metadata": {},
   "source": [
    "Now we can train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a1e5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, epochs, device, x_dim=784):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        overall_loss = 0\n",
    "        for batch_idx, (x, _) in enumerate(train_loader): # The enumerate also returns the labels, but we assign it to _ as they are not important right now.\n",
    "            x = x.view(batch_size, x_dim).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x_hat, mean, log_var = model(x)\n",
    "            #print(log_var)\n",
    "            loss = loss_function(x, x_hat, mean, log_var)\n",
    "            \n",
    "            overall_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"\\tEpoch\", epoch + 1, \"\\tAverage Loss: \", overall_loss/(batch_idx*batch_size))\n",
    "    return overall_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099d2d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, optimizer, epochs=50, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673c0945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function lets us generate digits from the decoder by providing a mean and variance.\n",
    "# The mean and variance of the latent space, which we will view momentarily, controls which\n",
    "# digit is likely to be produced.\n",
    "def generate_digit(mean, var):\n",
    "    z_sample = torch.tensor([[mean, var]], dtype=torch.float).to(device)\n",
    "    x_decoded = model.decode(z_sample)\n",
    "    digit = x_decoded.detach().cpu().reshape(28, 28) # reshape vector to 2d array\n",
    "    plt.imshow(digit, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2442e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_digit(0.0, 1.0), generate_digit(1.0, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62abca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent_space(model, scale=1.0, n=25, digit_size=28, figsize=15):\n",
    "    # display a n*n 2D manifold of digits\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "\n",
    "    # construct a grid \n",
    "    grid_x = np.linspace(-scale, scale, n)\n",
    "    grid_y = np.linspace(-scale, scale, n)[::-1]\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = torch.tensor([[xi, yi]], dtype=torch.float).to(device)\n",
    "            x_decoded = model.decode(z_sample)\n",
    "            digit = x_decoded[0].detach().cpu().reshape(digit_size, digit_size)\n",
    "            figure[i * digit_size : (i + 1) * digit_size, j * digit_size : (j + 1) * digit_size,] = digit\n",
    "\n",
    "    plt.figure(figsize=(figsize, figsize))\n",
    "    plt.title('VAE Latent Space Visualization')\n",
    "    start_range = digit_size // 2\n",
    "    end_range = n * digit_size + start_range\n",
    "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    plt.yticks(pixel_range, sample_range_y)\n",
    "    plt.xlabel(\"mean, z [0]\")\n",
    "    plt.ylabel(\"var, z [1]\")\n",
    "    plt.imshow(figure, cmap=\"Greys_r\")\n",
    "    plt.savefig(\"./high_res_figs/vae_latent_space_visual.png\", bbox_inches=\"tight\", dpi=500)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8389b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent_space(model, scale=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdf6c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also increase the scale that we want to plot to capture more of the latent space.\n",
    "plot_latent_space(model, scale=5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18cbb23",
   "metadata": {},
   "source": [
    "## Conditional VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f706fd43",
   "metadata": {},
   "source": [
    "A Conditional VAE  (CVAE) extends the standard Variational Autoencoder (VAE) by incorporating conditional information during training and generation. It takes input data and additional metadata (i.e., class labels, attributes, or styles) to learn a latent representation. This one will use convolutional neural networks for the encoder and decoder to provide a different example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b3f482",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    # Conditional VAE with CNNs in the encoder and decoder\n",
    "    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=200, num_class=10, device=device):\n",
    "        super(CVAE, self).__init__()\n",
    "\n",
    "        # encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1+1, 16, kernel_size=5, stride=2),  # First Conv2d layer generates batch_size 16 12x12 intermediates. The input channel is 1+1 because we concatenate the labels.\n",
    "            nn.ReLU(),                                    # ReLU activation\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=2),   # Second Conv2d layer generates batch_size 32 4x4 intermediates\n",
    "            nn.ReLU(),                                    # ReLU activation\n",
    "            nn.Flatten(),                                 # Reshapes the (batch_size, 32, 4, 4) tensor into (batch_size, 32*4*4) tensor\n",
    "            nn.Linear(32*4*4, hidden_dim),                # The input dimension is the flattened intermediates from the second Conv2d\n",
    "            nn.ReLU(),                                    # ReLU activation\n",
    "            nn.Linear(hidden_dim, latent_dim),            # Second layer transforms intermediate of 400 to an intermediate of 200    \n",
    "            nn.ReLU()                                     # ReLU activation\n",
    "            )\n",
    "\n",
    "        # latent mean and variance \n",
    "        self.mean_layer = nn.Linear(latent_dim, 2)     # Latent mean is found by transforming intermediate of 200 into a vector of 2\n",
    "        self.logvar_layer = nn.Linear(latent_dim, 2)   # Latent log-variance is found by transforming intermediate of 200 into a vector of 2\n",
    "\n",
    "        # Quick note, the variance is referred to as the log variance because log(s^2) = 2*log(s).\n",
    "        # The idea here is to avoid producing a negative variance.\n",
    "\n",
    "        # decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2+num_class, latent_dim),                    # First layer transforms reparameterization of 2+10 to an intermediate of 200\n",
    "            nn.ReLU(),                                             # ReLU activation\n",
    "            nn.Linear(latent_dim, hidden_dim),                     # Second layer transforms intermediate of 200 to an intermediate of 400\n",
    "            nn.ReLU(),                                             # ReLU activation\n",
    "            nn.Linear(hidden_dim, 32*4*4),                         # Third layer transforms intermediate of 400 into intermediate of 32*4*4\n",
    "            nn.ReLU(),                                             # ReLU activation\n",
    "            nn.Unflatten(1, (32, 4, 4)),                           # Reshape to (batch_size, 32, 4, 4)\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=5, stride=2),   # First Transpose generates batch_size 16 11x11 intermediates\n",
    "            nn.ReLU(),                                             # ReLU activation \n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=5, stride=2),    # Second Transpose generates batch_size 1 25x25 intermediates\n",
    "            nn.ReLU(),                                             # ReLU activation \n",
    "            nn.ConvTranspose2d(1, 1, kernel_size=4),               # Final Transpose generates 1 28x28 outputs\n",
    "            nn.Sigmoid()                                           # Sigmoid keeps images between 0 and 1\n",
    "            )\n",
    "\n",
    "    def encode(self, x, y):\n",
    "        y = torch.argmax(y, dim=1).reshape(y.shape[0], 1, 1, 1)  # Take the labels and reshape them to a Tensor of (batch_size, 1, 1, 1)\n",
    "        y = y.expand(-1, 1, x.shape[2], x.shape[3])              # Expand the labels to (batch_size, 1, 28, 28). Essentially copies the labels.\n",
    "        t = torch.cat((x, y), dim=1)                             # Creates the input of (batch_size, 1+1, 28, 28) for encoder\n",
    "        t = self.encoder(t)                                      # Pass the input into the encoder\n",
    "        mean, logvar = self.mean_layer(t), self.logvar_layer(t)  # Pass the output of the encoder into the mean and log-variance vectors\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterization(self, mean, std):\n",
    "        epsilon = torch.randn_like(std).to(device)  # Generates a vector from the N(0,1) distribution the same size of the variance vector\n",
    "        z = mean + std*epsilon                      # Reparameterization trick\n",
    "        return z\n",
    "\n",
    "    def decode(self, z, y):\n",
    "        z = torch.cat((z, y.float()), dim=1)   # Concatenate the one-hot encoded labels to the sampled vector.\n",
    "        return self.decoder(z)                 # Pass the input through the decoder.\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        mean, logvar = self.encode(x, y)                             # Pass the input into the encoder to obtain the mean and log-variance vectors\n",
    "        z = self.reparameterization(mean, torch.exp(0.5*logvar))     # Perform reparameterization trick, but not before converting log-variance back to variance.\n",
    "        x_hat = self.decode(z, y)                                    # Pass the sample into the decoder to obtain reconstruction\n",
    "        return x_hat, mean, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257a9fb3",
   "metadata": {},
   "source": [
    "Define our model and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1976920",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CVAE().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35d4b29",
   "metadata": {},
   "source": [
    "The loss function shouldn't need to be redefined because we're still comparing ground truth image to the image generated by the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b82435e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_CVAE(model, optimizer, epochs, device):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        overall_loss = 0\n",
    "        for batch_idx, (x, y) in enumerate(train_loader): # Unlike the VAE, we need the labels for training the CVAE.\n",
    "            x = x.view(batch_size, 1, 28, 28).to(device)\n",
    "            y = torch.nn.functional.one_hot(y, num_classes=10).to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x_hat, mean, log_var = model(x, y)\n",
    "            loss = loss_function(x, x_hat, mean, log_var)\n",
    "            \n",
    "            overall_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"\\tEpoch\", epoch + 1, \"\\tAverage Loss: \", overall_loss/(batch_idx*batch_size))\n",
    "    return overall_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e5c313",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_CVAE(model, optimizer, epochs=50, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2302bc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The generate digit is similar, but we must provide a OHE vector for the label... \n",
    "def generate_digit_CVAE(mean, var, label):\n",
    "    z_sample = torch.tensor([[mean, var]], dtype=torch.float).to(device)\n",
    "    label_ohe = torch.nn.functional.one_hot(torch.tensor([label]), num_classes=10)\n",
    "    x_decoded = model.decode(z_sample, label_ohe)\n",
    "    digit = x_decoded.detach().cpu().reshape(28, 28) # reshape vector to 2d array\n",
    "    plt.imshow(digit, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8f8b9a",
   "metadata": {},
   "source": [
    "Note how we can tell the CVAE to generate a specific number given a label, despite the mean and variance used. We may get digits of different qualities depending on the mean and variance used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5da9dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_digit_CVAE(0.0, 1.0, label=1), generate_digit_CVAE(0.0, 1.0, label=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cb37c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_digit_CVAE(1.0, 0.0, label=1), generate_digit_CVAE(1.0, 0.0, label=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a45840",
   "metadata": {},
   "source": [
    "In this example, using a mean of 0 and variance of 1 generates a different kind of '2' than a mean of 1 and variance of '0'. Let's take another look at how the latent space changes with labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb6eb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent_space_CVAE(model, scale=1.0, label=0, n=25, digit_size=28, figsize=15):\n",
    "    # display a n*n 2D manifold of digits\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "\n",
    "    # construct a grid \n",
    "    grid_x = np.linspace(-scale, scale, n)\n",
    "    grid_y = np.linspace(-scale, scale, n)[::-1]\n",
    "    \n",
    "    # one-hot encode the label.\n",
    "    label_ohe = torch.nn.functional.one_hot(torch.tensor([label]), num_classes=10)\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = torch.tensor([[xi, yi]], dtype=torch.float).to(device)\n",
    "            x_decoded = model.decode(z_sample, label_ohe)\n",
    "            digit = x_decoded[0].detach().cpu().reshape(digit_size, digit_size)\n",
    "            figure[i * digit_size : (i + 1) * digit_size, j * digit_size : (j + 1) * digit_size,] = digit\n",
    "\n",
    "    plt.figure(figsize=(figsize, figsize))\n",
    "    plt.title('CVAE Latent Space Visualization')\n",
    "    start_range = digit_size // 2\n",
    "    end_range = n * digit_size + start_range\n",
    "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    plt.yticks(pixel_range, sample_range_y)\n",
    "    plt.xlabel(\"mean, z [0]\")\n",
    "    plt.ylabel(\"var, z [1]\")\n",
    "    plt.imshow(figure, cmap=\"Greys_r\")\n",
    "    plt.savefig(\"./high_res_figs/cvae_latent_space_visual.png\", bbox_inches=\"tight\", dpi=500)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1792b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent_space_CVAE(model, scale=1.0, label=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d423ea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent_space_CVAE(model, scale=1.0, label=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aada25f5",
   "metadata": {},
   "source": [
    "We can clearly see that despite the changing mean and variance, that the CVAE produces similar images given a label. Let's see how the space changes when we up the scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629cf416",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent_space_CVAE(model, scale=5.0, label=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a514939",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent_space_CVAE(model, scale=5.0, label=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624a90f9",
   "metadata": {},
   "source": [
    "We can see that for smaller mean and variances, the CVAE generates higher quality digits. The quality decreases as we increase the mean and variances. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc08e504",
   "metadata": {},
   "source": [
    "## VAE Using LSTMs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bef3e8",
   "metadata": {},
   "source": [
    "For the ferroelectric capacitor project, I want to use a VAE to introduce stochasticity into the training and evaluation process because ferroelectric films are naturally stochastic. Before we can do that, let's find an example for how to create a VAE using LSTMs as the encoder and decoder, because the ferroelectric model shall rely on time series inputs.\n",
    "\n",
    "Although it is possible to learn MNIST through an LSTM, it is not very practical as that is not what LSTMs are designed to do. Instead, we will learn the MovingMNIST dataset. MovingMNIST contains 10,000 video sequences, each consisting of 20 frames. In each sequence, two digits move independently about the frame, which has a dimension of 64x64 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cc4220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch doesn't support MovingMNIST as well as it should. \n",
    "# If you do not already have it downloaded, run this line below.\n",
    "# Notice how we are downloading a NumPy file. We will have to process ourselves.\n",
    "! wget -q https://www.cs.toronto.edu/~nitish/unsupervised_video/mnist_test_seq.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25ac716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install these they are not already on your machine.\n",
    "import io\n",
    "import imageio\n",
    "from ipywidgets import widgets, HBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed1b76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the root to wherever the file downloaded to or wherever you move it.\n",
    "# We transpose such that the number of sequences comes before the number of frames.\n",
    "MovingMNIST = np.load('./data/MovingMNIST/mnist_test_seq.npy').transpose(1, 0, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee33e323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As mentioned, MovingMNIST has 10000 video sequences.\n",
    "# Each sequence has 20 frames.\n",
    "# Each frame is a 64x64 pixel image.\n",
    "MovingMNIST.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e5e9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data.\n",
    "np.random.shuffle(MovingMNIST)\n",
    "\n",
    "# Train, Test split (we are skipping validation...)\n",
    "# The default is a 90/10 (train/test) split.\n",
    "train_data = MovingMNIST[:9000]\n",
    "test_data = MovingMNIST[9000:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0658b75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use this function to collate the train set.\n",
    "def collate(batch):\n",
    "    # Add channel dim, scale pixels between 0 and 1, send to device\n",
    "    batch = torch.tensor(batch).unsqueeze(1)\n",
    "    batch = batch / 255.0\n",
    "    batch = batch.to(device)\n",
    "    \n",
    "    # Randomly pick 10 frames as the input (past), the 11th frame is the target (future)\n",
    "    rand = np.random.randint(10,20)\n",
    "    return batch[:,:,rand-10:rand], batch[:,:,rand]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d2f8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use this function to collate the test set.\n",
    "def collate_test(batch):\n",
    "    \n",
    "    # Last 10 frames are the target.\n",
    "    target = np.array(batch)[:,10:]\n",
    "    \n",
    "    # Add channel dim, scale pixels between 0 and 1, send to device\n",
    "    batch = torch.tensor(batch).unsqueeze(1)\n",
    "    batch = batch / 255.0\n",
    "    batch = batch.to(device)\n",
    "    return batch, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d9f223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "batch_size = 512\n",
    "input_size = 4096\n",
    "hidden_size = 2048\n",
    "latent_size = 1024\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False, collate_fn=collate_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48deb633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the data.\n",
    "# Get a batch\n",
    "input, _ = next(iter(train_loader))\n",
    "\n",
    "# Reverse process before displaying\n",
    "input = input.cpu().numpy() * 255.0\n",
    "\n",
    "for video in input.squeeze(1)[:3]:          # Loop over videos\n",
    "    with io.BytesIO() as gif:\n",
    "        imageio.mimsave(gif,video.astype(np.uint8),\"GIF\",fps=5)\n",
    "        display(HBox([widgets.Image(value=gif.getvalue())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61be1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input.squeeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fd6b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, sl, fd1, fd2 = input.squeeze(1).shape\n",
    "print(bs)\n",
    "print(sl)\n",
    "print(fd1)\n",
    "print(fd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7ad44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_squeezed = input.squeeze(1)\n",
    "input_squeezed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85c7c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input.view(bs, sl, -1).shape\n",
    "input_squeezed.reshape(bs, sl, -1).shape\n",
    "#imsize = input.squeeze(1).shape[2], input.squeeze(1).shape[3]\n",
    "#imsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500cbd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbce720e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample, _ = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf70518",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97d75b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just in case we need to redeclare the device again\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9714e7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be for visualizing the images\n",
    "def imshow(past_data, title=\"MovingMNIST\"):\n",
    "    num_img = len(past_data)\n",
    "    fig = fig = plt.figure(figsize=(4*num_img, 4))\n",
    "    \n",
    "    for idx in range(1, num_img+1):\n",
    "        ax = fig.add_subplot(1, num_img+1, idx)\n",
    "        ax.imshow(past_data[idx-1])\n",
    "    plt.suptitle(title, fontsize=30)\n",
    "    plt.savefig(f\"{title}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42bc9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMVAE(nn.Module):\n",
    "    # LSTM-based VAE\n",
    "    def __init__(self, input_dim=4096, hidden_dim=2048, latent_dim=1024, output_dim=4096, num_layers=1, device=device):\n",
    "        super(LSTMVAE, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.device = device\n",
    "\n",
    "        # encoder\n",
    "        self.encoder = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=False)\n",
    "        # The encoder in this case is simple. It's just a two-layer LSTM.\n",
    "\n",
    "        # latent mean and variance \n",
    "        self.mean_layer = nn.Linear(hidden_dim, latent_dim)     # Latent mean is found by transforming intermediate of 200 into a vector of 2\n",
    "        self.logvar_layer = nn.Linear(hidden_dim, latent_dim)   # Latent log-variance is found by transforming intermediate of 200 into a vector of 2\n",
    "\n",
    "        # init hidden\n",
    "        self.init_hidden = nn.Linear(latent_dim, hidden_dim)\n",
    "\n",
    "        # decoder\n",
    "        self.decoder1 = nn.LSTM(latent_dim, hidden_dim, num_layers, batch_first=True, bidirectional=False)\n",
    "        self.decoder2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        enc_outputs, enc_hidden = self.encoder(x)   # Pass the input into the encoder\n",
    "        #print(enc_hidden[0].shape)\n",
    "        enc_h = enc_hidden[0].view(self.batch_size, self.hidden_dim).to(self.device)  # Obtains the hidden state\n",
    "        mean, logvar = self.mean_layer(enc_h), self.logvar_layer(enc_h)   # Pass the output of the encoder into the mean and log-variance vectors\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterization(self, mean, std):\n",
    "        epsilon = torch.randn_like(std).to(device)  # Generates a vector from the N(0,1) distribution the same size of the variance vector\n",
    "        z = mean + std*epsilon                      # Reparameterization trick\n",
    "        return z\n",
    "    \n",
    "    def decode(self, z, hidden):\n",
    "        dec_outputs, (dec_hidden, dec_cell) = self.decoder1(z, hidden)\n",
    "        prediction = self.decoder2(dec_outputs)\n",
    "        return prediction, (dec_hidden, dec_cell)              # Pass the input through the decoder.\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.batch_size, seq_len, features = x.shape\n",
    "        \n",
    "        mean, logvar = self.encode(x)                                # Pass the input into the encoder to obtain the mean and log-variance vectors\n",
    "        z = self.reparameterization(mean, torch.exp(0.5*logvar))     # Perform reparameterization trick, but not before converting log-variance back to variance.\n",
    "        h_ = self.init_hidden(z)\n",
    "        h_ = h_.unsqueeze(0) # added\n",
    "        z = z.repeat(1, seq_len, 1)\n",
    "        z = z.view(self.batch_size, seq_len, self.latent_dim).to(device)\n",
    "        hidden = (h_.contiguous(), h_.contiguous())\n",
    "        #print(h_.shape)\n",
    "        reconstruct_output, hidden = self.decode(z, hidden)          # Pass the sample into the decoder to obtain reconstruction\n",
    "        x_hat = reconstruct_output\n",
    "        return x_hat, mean, logvar    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b735fa3f",
   "metadata": {},
   "source": [
    "Define the model and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15c6dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMVAE().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ae76f8",
   "metadata": {},
   "source": [
    "We may have to change the loss function slightly for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490afaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_LSTM(x, x_hat, mean, log_var):\n",
    "    kld_weight = 2.5e-4 # Account for the minibatch samples in the dataset\n",
    "    reproduction_loss = nn.functional.mse_loss(x_hat, x)\n",
    "    KLD = torch.mean(-0.5*torch.sum(1+log_var - mean.pow(2) - log_var.exp(), dim=1), dim=0)\n",
    "    return reproduction_loss + kld_weight*KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5224f1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 1000\n",
    "epochs = max_iters // len(train_loader)+1\n",
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722acbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1675fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTMVAE(model, optimizer, epochs, device):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        overall_loss = 0\n",
    "        for batch_idx, (past_data, future_data) in enumerate(train_loader):\n",
    "            # Get rid of the extra dimension from our collate function.\n",
    "            past_data = past_data.squeeze(1)\n",
    "            \n",
    "            batch_size = past_data.size(0)\n",
    "            example_size = past_data.size(1)\n",
    "            image_size = past_data.size(2), past_data.size(3)\n",
    "            past_data = (\n",
    "                past_data.reshape(batch_size, example_size, -1).float().to(device)\n",
    "            )\n",
    "            # We aren't concerned about future data.\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            x_hat, mean, log_var = model(past_data)\n",
    "            loss = loss_function_LSTM(past_data, x_hat, mean, log_var)\n",
    "            \n",
    "            overall_loss += loss.mean().item()\n",
    "            \n",
    "            loss.mean().backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(\"\\tEpoch\", epoch + 1, \"\\tAverage Loss: \", overall_loss/len(train_loader))\n",
    "        \n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (past_data, future_data) in enumerate(test_loader):\n",
    "                # Get rid of the extra dimension from our collate function.\n",
    "                past_data = past_data.squeeze(1)\n",
    "                \n",
    "                batch_size = past_data.size(0)\n",
    "                example_size = past_data.size(1)\n",
    "                image_size = past_data.size(2), past_data.size(3)\n",
    "                past_data = (\n",
    "                    past_data.reshape(batch_size, example_size, -1).float().to(device)\n",
    "                )\n",
    "                \n",
    "                x_hat, mean, log_var = model(past_data)\n",
    "                \n",
    "                loss = loss_function_LSTM(past_data, x_hat, mean, log_var)\n",
    "                eval_loss += loss.mean().item()\n",
    "                \n",
    "                if (batch_idx == 0):\n",
    "                    nhw_orig = past_data[0].view(example_size, image_size[0], -1)\n",
    "                    nhw_recon = x_hat[0].view(example_size, image_size[0], -1)\n",
    "                    imshow(nhw_orig.cpu(), f\"orig{epoch}\")\n",
    "                    imshow(nhw_recon.cpu(), f\"recon{epoch}\")\n",
    "                    \n",
    "        eval_loss = eval_loss / len(test_loader)\n",
    "        print(\"\\tEpoch\", epoch + 1, \"\\tEvaluation Score: \", eval_loss)\n",
    "    return overall_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8e4268",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913eb3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = torch.randn((512, 2048))\n",
    "dummy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae685e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be48e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ = (dummy.contiguous(), dummy.contiguous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7c661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec422a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LSTMVAE(model, optimizer, epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518da1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "\n",
    "# Define the URL where the dataset can be downloaded\n",
    "url = 'http://www.cs.toronto.edu/~nitish/unsupervised_video/mnist_test_seq.npy'\n",
    "\n",
    "# Define the path where the dataset should be saved\n",
    "save_path = './data/MovingMNIST/mnist_test_seq.npy'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "# Download the dataset\n",
    "urllib.request.urlretrieve(url, save_path)\n",
    "\n",
    "# Verify the file has been downloaded\n",
    "assert os.path.isfile(save_path), \"File download failed\"\n",
    "\n",
    "# Load the dataset\n",
    "MovingMNIST = np.load(save_path).transpose(1, 0, 2, 3)\n",
    "\n",
    "# Verify the shape of the dataset\n",
    "print(\"Dataset shape:\", MovingMNIST.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
